
[![Model License](https://img.shields.io/badge/Model%20License-GPL_v3.0-green.svg)]()
[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-red.svg)]()
![](https://img.shields.io/github/last-commit/ydli-ai/Chinese-ChatLLaMA)
![](https://img.shields.io/github/commit-activity/m/ydli-ai/Chinese-ChatLLaMA)
![](https://img.shields.io/github/languages/top/ydli-ai/Chinese-ChatLLaMA)
![](https://img.shields.io/github/stars/ydli-ai/Chinese-ChatLLaMA?style=social)



æœ¬é¡¹ç›®å‘ç¤¾åŒºæä¾›ä¸­æ–‡å¯¹è¯æ¨¡å‹ ChatLLama ã€ä¸­æ–‡åŸºç¡€æ¨¡å‹ LLaMA-zh åŠå…¶è®­ç»ƒæ•°æ®ã€‚
æ¨¡å‹åŸºäº [TencentPretrain](https://github.com/Tencent/TencentPretrain) å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶æ„å»ºï¼Œ å°†é™†ç»­å¼€æ”¾ 7Bã€13Bã€33Bã€65B è§„æ¨¡çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹ LLaMA-zh æƒé‡ã€‚
    
ChatLLaMA æ”¯æŒç®€ç¹ä½“ä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ç­‰å¤šè¯­è¨€ã€‚
LLaMA åœ¨é¢„è®­ç»ƒé˜¶æ®µä¸»è¦ä½¿ç”¨è‹±æ–‡ï¼Œä¸ºäº†å°†å…¶è¯­è¨€èƒ½åŠ›è¿ç§»åˆ°ä¸­æ–‡ä¸Šï¼Œé¦–å…ˆè¿›è¡Œä¸­æ–‡å¢é‡é¢„è®­ç»ƒï¼Œ
ä½¿ç”¨çš„è¯­æ–™åŒ…æ‹¬[ä¸­è‹±å¹³è¡Œè¯­æ–™](https://statmt.org/wmt18/translation-task.html#download)ã€[ä¸­æ–‡ç»´åŸºã€ç¤¾åŒºäº’åŠ¨ã€æ–°é—»æ•°æ®](https://github.com/CLUEbenchmark/CLUECorpus2020)ã€[ç§‘å­¦æ–‡çŒ®](https://github.com/ydli-ai/CSL)ç­‰ã€‚å†é€šè¿‡ [Alpaca æŒ‡ä»¤å¾®è°ƒ](https://github.com/tatsu-lab/stanford_alpaca)å¾—åˆ° Chinese-ChatLLaMAã€‚

**é¡¹ç›®ç‰¹ç‚¹**
+ é€šè¿‡ Full-tuning ï¼ˆå…¨å‚æ•°è®­ç»ƒï¼‰è·å¾—ä¸­æ–‡æ¨¡å‹æƒé‡ï¼Œæä¾› TencentPretrain ä¸ HuggingFace ç‰ˆæœ¬
+ æ¨¡å‹ç»†èŠ‚å…¬å¼€å¯å¤ç°ï¼Œæä¾›æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œæ¨¡å‹è¯„ä¼°å®Œæ•´æµç¨‹ä»£ç 
+ æä¾›ç›®å‰æœ€å¤§çš„ä¸­æ–‡ LLaMA æ¨¡å‹
+ å¤šç§é‡åŒ–æ–¹æ¡ˆï¼Œæ”¯æŒ CUDA å’Œè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²æ¨ç†

[ä¸­æ–‡é¢„è®­ç»ƒè¯­æ–™](corpus/README.md) | [ä¸­æ–‡æŒ‡ä»¤ç²¾è°ƒæ•°æ®é›†](instructions/README.md) | [æ¨¡å‹é‡åŒ–éƒ¨ç½²](https://github.com/fengyh3/llama_inference) | [é¢†åŸŸå¾®è°ƒç¤ºä¾‹](#todo-list)

## News

+ **[2023/4/17]** [llama_inference](https://github.com/fengyh3/llama_inference) æ›´æ–° 8-bit é‡åŒ–æ¨ç†å’Œå¾®æœåŠ¡éƒ¨ç½²ï¼Œå¤§å¹…åº¦æå‡æ¨ç†é€Ÿåº¦å¹¶é™ä½å†…å­˜æ¶ˆè€—

+ **[2023/4/8]** [TencentPretrain](https://github.com/Tencent/TencentPretrain) ç°å·²æ”¯æŒ LoRA è®­ç»ƒå’Œ DeepSpeed Zero-3 Offload æµæ°´çº¿å¹¶è¡Œ 

+ **[2023/4/1]** æ›´æ–° 4-bit é‡åŒ–ç‰ˆæœ¬ ChatLLaMA æ¨¡å‹æƒé‡ï¼Œæ”¯æŒ [llama.cpp](https://github.com/ggerganov/llama.cpp) é«˜é€Ÿæ¨ç†

+ **[2023/3/28]** å¼€æ”¾åŸºäº LLaMA çš„ä¸­æ–‡å¯¹è¯æ¨¡å‹ ChatLLaMA-zh-7B ï¼Œ [æŠ€æœ¯åšå®¢](https://zhuanlan.zhihu.com/p/616748134)

-----

## ç›®å½•

+ [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½) 
+ [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
+ [æ¨¡å‹è®­ç»ƒ](#æ¨¡å‹è®­ç»ƒ)
+ [ç”Ÿæˆç¤ºä¾‹](#ç”Ÿæˆç¤ºä¾‹)
+ [å±€é™æ€§](#å±€é™æ€§)
+ [ä¸­æ–‡é¢„è®­ç»ƒ/æŒ‡ä»¤æ•°æ®é›†](#ä¸­æ–‡é¢„è®­ç»ƒ/æŒ‡ä»¤æ•°æ®é›†)
+ [äº¤æµå’Œé—®é¢˜åé¦ˆ](#äº¤æµå’Œé—®é¢˜åé¦ˆ)
+ [TODO-List](#todo-list)
+ [License](#License)
+ [Contributors](#Contributors)


## æ¨¡å‹ä¸‹è½½

**ä½¿ç”¨é¡»çŸ¥** âš ï¸ 

æ¨¡å‹æƒé‡åŸºäº [GNU General Public License v3.0](https://www.gnu.org/licenses/gpl-3.0.html) åè®®ï¼Œä»…ä¾›ç ”ç©¶ä½¿ç”¨ï¼Œä¸èƒ½ç”¨äºå•†ä¸šç›®çš„ã€‚
è¯·ç¡®è®¤åœ¨å·²[è·å¾—è®¸å¯](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form)çš„å‰æä¸‹ä½¿ç”¨æœ¬ä»“åº“ä¸­çš„æ¨¡å‹ã€‚


**7B**ï¼š[åŸºç¡€æ¨¡å‹ LLaMA-zh-7B](https://huggingface.co/P01son/LLaMA-zh-7B/)ï½œ [å¯¹è¯æ¨¡å‹ ChatLLaMA-zh-7B](https://huggingface.co/P01son/ChatLLaMA-zh-7B)ï½œ [int4é‡åŒ–ç‰ˆæœ¬ ChatLLaMA](https://huggingface.co/P01son/ChatLLaMA-zh-7B-int4)   
**13B**ï¼š[åŸºç¡€æ¨¡å‹ LLaMA-zh-13B](https://huggingface.co/P01son/LLaMA-zh-13B)ï½œ [å¯¹è¯æ¨¡å‹ ChatLLaMA-zh-13BğŸ”¥](https://huggingface.co/P01son/ChatLLaMA-zh-13B/)  
**33B**ï¼šä¸Šä¼ ä¸­  
**65B**ï¼šè§„åˆ’ä¸­

<!-- 
ğŸ¤—**HuggingFaceæ¨¡å‹**  
[7B åŸºç¡€æ¨¡å‹]()ï½œ [7B å¯¹è¯æ¨¡å‹]() | [13B åŸºç¡€æ¨¡å‹]() -->

### è®­ç»ƒæƒ…å†µ

æ¨¡å‹ä»åœ¨è¿­ä»£ä¸­ï¼Œæ¯å‘¨æ›´æ–°ä¸€æ¬¡æ–°ç‰ˆæ¨¡å‹æƒé‡ã€‚
<center class="half">
    <img src="assets/loss.png" width="500"/><img src="assets/acc.png" width="500"/>
</center>


## å¿«é€Ÿå¼€å§‹

ä¸‹è½½é¢„è®­ç»ƒ ChatLLaMA æƒé‡ï¼Œå®‰è£…ä¾èµ–ï¼Œæµ‹è¯•ç¯å¢ƒ: py3.8.12 cuda11.2.2 cudnn8.1.1.33-1 torch1.9.0 bitsandbytes0.37.2

```bash
git lfs install
git clone https://huggingface.co/P01son/ChatLLaMA-zh-7B
git clone https://github.com/fengyh3/llama_inference.git

cd llama_inference 
vi beginning.txt  #ç¼–è¾‘ç”¨æˆ·è¾“å…¥ï¼Œä¾‹å¦‚"ä¸Šæµ·æœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹ï¼Ÿ"

python3 llama_infer.py --test_path prompts.txt --prediction_path result.txt  \
                      --load_model_path ../ChatLLaMA-zh-7B/chatllama_7b.bin  \
                      --config_path config/llama_7b_config.json \
                      --spm_model_path ../ChatLLaMA-zh-7B/tokenizer.model --seq_length 512
```

### å¤šè½®å¯¹è¯

TODO

### Int8 æ¨ç†åŠ é€Ÿ

```bash
python3 llama_infer.py --test_path prompts.txt --prediction_path result.txt  \
                      --load_model_path ../ChatLLaMA-zh-7B/chatllama_7b.bin  \
                      --config_path config/llama_7b_config.json \
                      --spm_model_path ../ChatLLaMA-zh-7B/tokenizer.model --seq_length 512 --use_int8 
```

### å¾®æœåŠ¡éƒ¨ç½²

å®‰è£…ä¾èµ–ï¼šflask
```bash
python3 llama_server.py --load_model_path ../ChatLLaMA-zh-7B/chatllama_7b.bin  \
                        --config_path config/llama_7b_config.json \
                        --spm_model_path ../ChatLLaMA-zh-7B/tokenizer.model --seq_length 512

curl -H 'Content-Type: application/json' http://127.0.0.1:8888/chat -d '{"question": "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹ï¼Ÿ"}'
```


### Int4 CPUæœ¬åœ°éƒ¨ç½²

å°† Int4 é‡åŒ–åçš„æ¨¡å‹æƒé‡éƒ¨ç½²åœ¨æœ¬åœ°ä½¿ç”¨CPUæ¨ç†ã€‚

```bash
git lfs install
git clone https://github.com/ggerganov/llama.cpp.git
git clone https://huggingface.co/P01son/ChatLLaMA-zh-7B-int4

cd llama.cpp
make
./main -m ../ChatLLaMA-zh-7B-int4/chatllama-ggml-q4_0.bin -p "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹ï¼Ÿ\n" -n 256
```


## æ¨¡å‹è®­ç»ƒ

å®‰è£…ä¾èµ–ï¼Œæµ‹è¯•ç¯å¢ƒ: py3.8.12 cuda11.2.2 cudnn8.1.1.33-1 nccl2.10.3 deepspeed0.8.3 torch1.9.0

ä½¿ç”¨ TencentPretrain è®­ç»ƒï¼š
```
git clone https://github.com/Tencent/TencentPretrain.git
cd TencentPretrain

#å°† tencentpretrain/utils/constants.py æ–‡ä»¶ä¸­ L4: special_tokens_map.json ä¿®æ”¹ä¸º llama_special_tokens_map.json
```

### ä¸­æ–‡å¢é‡é¢„è®­ç»ƒ

ä»¥ 7B æ¨¡å‹ä¸ºä¾‹ï¼Œé¦–å…ˆä¸‹è½½[é¢„è®­ç»ƒLLaMAæƒé‡](https://huggingface.co/decapoda-research/llama-7b-hf)ï¼Œè½¬æ¢åˆ°TencentPretrainæ ¼å¼ï¼š

```
python3 scripts/convert_llama_from_huggingface_to_tencentpretrain.py --input_model_path $LLaMA_HF_PATH \
                       --output_model_path  models/llama-7b.bin --type 7B
```

ä¸‹è½½[ä¸­æ–‡é¢„è®­ç»ƒè¯­æ–™](corpus/README.md)ï¼Œ
é¢„å¤„ç†ï¼š

```
python3 preprocess.py --corpus_path $CORPUS_PATH --spm_model_path $LLaMA_PATH/tokenizer.model \
                      --dataset_path $OUTPUT_DATASET_PATH --data_processor lm --seq_length 512
```

é¢„è®­ç»ƒï¼š

```
deepspeed pretrain.py --deepspeed --deepspeed_config models/deepspeed_zero3_config.json --enable_zero3 \
                      --pretrained_model_path models/llama-7b.bin \
                      --dataset_path $OUTPUT_DATASET_PATH --spm_model_path $LLaMA_PATH/tokenizer.model \
                      --config_path models/llama/7b_config.json \
                      --output_model_path models/llama_zh_7b \
                      --world_size 8 --data_processor lm \
                      --total_steps 300000 --save_checkpoint_steps 5000 --batch_size 24
```



### ä¸­æ–‡æŒ‡ä»¤å­¦ä¹ 

æ„å»º[æŒ‡ä»¤æ•°æ®é›†](instructions/README.md)å¹¶é¢„å¤„ç†ï¼š

```
python3 preprocess.py --corpus_path $INSTRUCTION_PATH --spm_model_path $LLaMA_PATH/tokenizer.model \
                      --dataset_path $OUTPUT_DATASET_PATH --data_processor alpaca --seq_length 512
```

æŒ‡ä»¤å¾®è°ƒï¼š

```
deepspeed pretrain.py --deepspeed --deepspeed_config models/deepspeed_zero3_config.json --enable_zero3 \
                      --pretrained_model_path models/llama_zh_7b.bin \
                      --dataset_path $OUTPUT_DATASET_PATH --spm_model_path $LLaMA_PATH/tokenizer.model \
                      --config_path models/llama/7b_config.json \
                      --output_model_path models/chatllama_7b \
                      --world_size 8 --data_processor alpaca \
                      --total_steps 20000 --save_checkpoint_steps 2000 --batch_size 24
```



## ç”Ÿæˆç¤ºä¾‹


<details>
<summary><b>å¸¸è¯†æ¨ç†</b></summary>

TODO

</details>


<details>
<summary><b>é€»è¾‘æ¨ç†</b></summary>

TODO

</details>


<details>
<summary><b>çŸ¥è¯†é—®ç­”</b></summary>

TODO

</details>

<details>
<summary><b>è¯­ä¹‰ç†è§£</b></summary>

TODO

</details>

<details>
<summary><b>æ•°å€¼è®¡ç®—</b></summary>

TODO

</details>

<details>
<summary><b>è¯—æ–‡å†™ä½œ</b></summary>

TODO

</details>

<details>
<summary><b>æ–‡æœ¬ç¿»è¯‘</b></summary>

TODO

</details>

<details>
<summary><b>æ­§ä¹‰ç†è§£</b></summary>

TODO

</details>

## å±€é™æ€§

ChatLLaMA å®Œå…¨åŸºäºç¤¾åŒºå¼€æ”¾è¯­æ–™è®­ç»ƒï¼Œå†…å®¹æœªç»äººå·¥ä¿®æ­£ã€‚å—é™äºæ¨¡å‹å’Œè®­ç»ƒæ•°æ®è§„æ¨¡ï¼ŒChatLLaMA çš„è¯­è¨€èƒ½åŠ›è¾ƒå¼±ï¼Œ
åœ¨å¤šè½®å¯¹è¯ã€é€»è¾‘æ¨ç†ã€çŸ¥è¯†é—®ç­”ç­‰åœºæ™¯å…·æœ‰æ˜æ˜¾ç¼ºé™·ï¼Œä¹Ÿå¯èƒ½äº§ç”Ÿå¸¦æœ‰åè§æˆ–æœ‰å®³å†…å®¹ã€‚

## ä¸­æ–‡é¢„è®­ç»ƒ/æŒ‡ä»¤æ•°æ®é›†

æ±‡æ€»å¼€æºç¤¾åŒºæ„å»ºçš„ä¸­æ–‡æŒ‡ä»¤å­¦ä¹ æ•°æ®ï¼Œå¹¶è½¬æ¢æˆç»Ÿä¸€æ ¼å¼ç”¨äºæŒ‡ä»¤å¾®è°ƒã€‚ 

[æ„å»ºä¸­](instructions/README.md)


## äº¤æµå’Œé—®é¢˜åé¦ˆ

ç”±äºå¾®ä¿¡ç¾¤è¾¾åˆ°äººæ•°ä¸Šé™ï¼Œæœç´¢å¾®ä¿¡å· chatllamaï¼Œæ·»åŠ ä¸ºå¥½å‹åæ‹‰å…¥ç¾¤èŠã€‚

## TODO List

- [ ] HuggingFace è½¬æ¢è„šæœ¬å’Œæƒé‡ä¸Šä¼ 
- [ ] æ”¯æŒé‡åŒ–æ¨¡å‹ CUDA éƒ¨ç½²
- [ ] ChatLLaMA é¢†åŸŸé€‚åº”æ¡ˆä¾‹
- [ ] å¼ºåŒ–å­¦ä¹ 

## License

Our code and documents are released under Apache Licence 2.0

Following LLaMA, our pre-trained weights are released under GNU General Public License v3.0

## Contributors

We thank contributors for both [TencentPretrain](https://github.com/Tencent/TencentPretrain) and Chinese-ChatLLaMA projects.

Authors: [Yudong Li](https://github.com/ydli-ai), [Zhe Zhao](https://github.com/zhezhaoa), [Yuhao Feng](https://github.com/fengyh3), [Cheng Hou](https://github.com/hhou435), [Shuang Li](https://github.com/thulishuang), [Hao Li](https://github.com/wmpscc), [Xianxu Hou](https://houxianxu.github.io/)

Corresponding Authors: [Linlin Shen](https://scholar.google.com/citations?user=AZ_y9HgAAAAJ&hl=zh-CN), Kimmo Yan

