
[![Model License](https://img.shields.io/badge/Model%20License-GPL_v3.0-green.svg)]()
[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-red.svg)]()
![](https://img.shields.io/github/last-commit/ydli-ai/Chinese-ChatLLaMA)
![](https://img.shields.io/github/commit-activity/m/ydli-ai/Chinese-ChatLLaMA)
![](https://img.shields.io/github/languages/top/ydli-ai/Chinese-ChatLLaMA)
![](https://img.shields.io/github/stars/ydli-ai/Chinese-ChatLLaMA?style=social)



æœ¬é¡¹ç›®å‘ç¤¾åŒºæä¾›ä¸­æ–‡å¯¹è¯æ¨¡å‹ ChatLLama ã€ä¸­æ–‡åŸºç¡€æ¨¡å‹ LLaMA-zh åŠå…¶è®­ç»ƒæ•°æ®ã€‚
æ¨¡å‹åŸºäº [TencentPretrain](https://github.com/Tencent/TencentPretrain) å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶æ„å»ºï¼Œ å°†é™†ç»­å¼€æ”¾ 7Bã€13Bã€33Bã€65B è§„æ¨¡çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹ LLaMA-zh æƒé‡ã€‚
    
ChatLLaMA æ”¯æŒç®€ç¹ä½“ä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ç­‰å¤šè¯­è¨€ã€‚
LLaMA åœ¨é¢„è®­ç»ƒé˜¶æ®µä¸»è¦ä½¿ç”¨è‹±æ–‡ï¼Œä¸ºäº†å°†å…¶è¯­è¨€èƒ½åŠ›è¿ç§»åˆ°ä¸­æ–‡ä¸Šï¼Œé¦–å…ˆè¿›è¡Œä¸­æ–‡å¢é‡é¢„è®­ç»ƒï¼Œ
ä½¿ç”¨çš„è¯­æ–™åŒ…æ‹¬[ä¸­è‹±å¹³è¡Œè¯­æ–™](https://statmt.org/wmt18/translation-task.html#download)ã€[ä¸­æ–‡ç»´åŸºã€ç¤¾åŒºäº’åŠ¨ã€æ–°é—»æ•°æ®](https://github.com/CLUEbenchmark/CLUECorpus2020)ã€[ç§‘å­¦æ–‡çŒ®](https://github.com/ydli-ai/CSL)ç­‰ã€‚å†é€šè¿‡ [Alpaca æŒ‡ä»¤å¾®è°ƒ](https://github.com/tatsu-lab/stanford_alpaca)å¾—åˆ° Chinese-ChatLLaMAã€‚

**é¡¹ç›®ç‰¹ç‚¹**
+ é€šè¿‡ Full-tuning ï¼ˆå…¨å‚æ•°è®­ç»ƒï¼‰è·å¾—ä¸­æ–‡æ¨¡å‹æƒé‡ï¼Œæä¾› TencentPretrain ä¸ HuggingFace ç‰ˆæœ¬
+ æ¨¡å‹ç»†èŠ‚å…¬å¼€å¯å¤ç°ï¼Œæä¾›æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œæ¨¡å‹è¯„ä¼°å®Œæ•´æµç¨‹ä»£ç 
+ æä¾›ç›®å‰æœ€å¤§çš„ä¸­æ–‡ LLaMA æ¨¡å‹
+ å¤šç§é‡åŒ–æ–¹æ¡ˆï¼Œæ”¯æŒ CUDA å’Œè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²æ¨ç†

[ä¸­æ–‡é¢„è®­ç»ƒè¯­æ–™](corpus/README.md) | [ä¸­æ–‡æŒ‡ä»¤ç²¾è°ƒæ•°æ®é›†](instructions/README.md) | [æ¨¡å‹é‡åŒ–éƒ¨ç½²](https://github.com/fengyh3/llama_inference) | [é¢†åŸŸå¾®è°ƒç¤ºä¾‹](#todo-list)

## News

+ **[2023/4/17]** [llama_inference](https://github.com/fengyh3/llama_inference) æ›´æ–° 8-bit é‡åŒ–æ¨ç†å’Œå¾®æœåŠ¡éƒ¨ç½²ï¼Œå¤§å¹…åº¦æå‡æ¨ç†é€Ÿåº¦å¹¶é™ä½å†…å­˜æ¶ˆè€—

+ **[2023/4/8]** [TencentPretrain](https://github.com/Tencent/TencentPretrain) ç°å·²æ”¯æŒ LoRA è®­ç»ƒå’Œ DeepSpeed Zero-3 Offload æµæ°´çº¿å¹¶è¡Œ 

+ **[2023/4/1]** æ›´æ–° 4-bit é‡åŒ–ç‰ˆæœ¬ ChatLLaMA æ¨¡å‹æƒé‡ï¼Œæ”¯æŒ [llama.cpp](https://github.com/ggerganov/llama.cpp) é«˜é€Ÿæ¨ç†

+ **[2023/3/28]** å¼€æ”¾åŸºäº LLaMA çš„ä¸­æ–‡å¯¹è¯æ¨¡å‹ ChatLLaMA-zh-7B ï¼Œ [æŠ€æœ¯åšå®¢](https://zhuanlan.zhihu.com/p/616748134)

-----

## ç›®å½•

+ [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½) 
+ [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
+ [æ¨¡å‹è®­ç»ƒ](#æ¨¡å‹è®­ç»ƒ)
+ [ç”Ÿæˆç¤ºä¾‹](#ç”Ÿæˆç¤ºä¾‹)
+ [å±€é™æ€§](#å±€é™æ€§)
+ [ä¸­æ–‡é¢„è®­ç»ƒ/æŒ‡ä»¤æ•°æ®é›†](#ä¸­æ–‡é¢„è®­ç»ƒ/æŒ‡ä»¤æ•°æ®é›†)
+ [äº¤æµå’Œé—®é¢˜åé¦ˆ](#äº¤æµå’Œé—®é¢˜åé¦ˆ)
+ [TODO-List](#todo-list)
+ [License](#License)
+ [Contributors](#Contributors)


## æ¨¡å‹ä¸‹è½½

**ä½¿ç”¨é¡»çŸ¥** âš ï¸ 

æ¨¡å‹æƒé‡åŸºäº [GNU General Public License v3.0](https://www.gnu.org/licenses/gpl-3.0.html) åè®®ï¼Œä»…ä¾›ç ”ç©¶ä½¿ç”¨ï¼Œä¸èƒ½ç”¨äºå•†ä¸šç›®çš„ã€‚
è¯·ç¡®è®¤åœ¨å·²[è·å¾—è®¸å¯](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form)çš„å‰æä¸‹ä½¿ç”¨æœ¬ä»“åº“ä¸­çš„æ¨¡å‹ã€‚


**7B**ï¼š[åŸºç¡€æ¨¡å‹ LLaMA_zh]()ï½œ [å¯¹è¯æ¨¡å‹ ChatLLaMAğŸ”¥](https://huggingface.co/P01son/ChatLLaMA-zh-7B)ï½œ [int4é‡åŒ–ç‰ˆæœ¬ ChatLLaMA](https://huggingface.co/P01son/ChatLLaMA-zh-7B-int4)   
**13B**ï¼šé¢„è®¡ ~~4æœˆ11æ—¥~~ 4æœˆ20æ—¥å…¬å¼€  
**33B**ï¼šåŸºç¡€æ¨¡å‹é¢„è®¡4æœˆ20æ—¥å…¬å¼€  
**65B**ï¼šè§„åˆ’ä¸­

<!-- 
ğŸ¤—**HuggingFaceæ¨¡å‹**  
[7B åŸºç¡€æ¨¡å‹]()ï½œ [7B å¯¹è¯æ¨¡å‹]() | [13B åŸºç¡€æ¨¡å‹]() -->


æ¨¡å‹ä»åœ¨è¿­ä»£ä¸­ï¼Œæ¯å‘¨æ›´æ–°ä¸€æ¬¡æ–°ç‰ˆæ¨¡å‹æƒé‡ã€‚

## å¿«é€Ÿå¼€å§‹

ä¸‹è½½é¢„è®­ç»ƒ ChatLLaMA æƒé‡ï¼Œå®‰è£…ä¾èµ–ï¼Œæµ‹è¯•ç¯å¢ƒ: py3.8.12 cuda11.2.2 cudnn8.1.1.33-1 torch1.9.0 bitsandbytes0.37.2

```bash
git lfs install
git clone https://huggingface.co/P01son/ChatLLaMA-zh-7B
git clone https://github.com/fengyh3/llama_inference.git

cd llama_inference 
vi beginning.txt  #ç¼–è¾‘ç”¨æˆ·è¾“å…¥ï¼Œä¾‹å¦‚"ä¸Šæµ·æœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹ï¼Ÿ"

#å°†é¡¹ç›®ä¸­çš„ generate_chatllama.py å¤åˆ¶åˆ° scripts/

python3 llama_infer.py --test_path prompts.txt --prediction_path result.txt  \
                      --load_model_path ../ChatLLaMA-zh-7B/chatllama_7b.bin  \
                      --config_path config/llama_7b_config.json \
                      --spm_model_path ../ChatLLaMA-zh-7B/tokenizer.model --seq_length 512
```

### å¤šè½®å¯¹è¯

TODO

### 4-bit æ¨ç†åŠ é€Ÿ

```bash
python3 llama_infer.py --test_path prompts.txt --prediction_path result.txt  \
                      --load_model_path ../ChatLLaMA-zh-7B/chatllama_7b.bin  \
                      --config_path config/llama_7b_config.json \
                      --spm_model_path ../ChatLLaMA-zh-7B/tokenizer.model --seq_length 512 --use_int8 
```

### å¾®æœåŠ¡éƒ¨ç½²

å®‰è£…ä¾èµ–ï¼šflask
```bash
python3 llama_server.py --load_model_path ../ChatLLaMA-zh-7B/chatllama_7b.bin  \
                        --config_path config/llama_7b_config.json \
                        --spm_model_path ../ChatLLaMA-zh-7B/tokenizer.model --seq_length 512

curl -H 'Content-Type: application/json' http://127.0.0.1:8888/chat -d '{"question": "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹ï¼Ÿ"}'
```


### 5-bit CPUæœ¬åœ°éƒ¨ç½²

å°†int4é‡åŒ–åçš„æ¨¡å‹æƒé‡éƒ¨ç½²åœ¨æœ¬åœ°ä½¿ç”¨CPUæ¨ç†ã€‚

```bash
git lfs install
git clone https://github.com/ggerganov/llama.cpp.git
git clone https://huggingface.co/P01son/ChatLLaMA-zh-7B-int4

cd llama.cpp
make
./main -m ../ChatLLaMA-zh-7B-int4/chatllama-ggml-q4_0.bin -p "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹ï¼Ÿ\n" -n 256
```


## æ¨¡å‹è®­ç»ƒ

å®‰è£…ä¾èµ–ï¼Œæµ‹è¯•ç¯å¢ƒ: py3.8.12 cuda11.2.2 cudnn8.1.1.33-1 nccl2.10.3 deepspeed0.8.3 torch1.9.0

### ä¸­æ–‡å¢é‡é¢„è®­ç»ƒ

ä»¥ 7B æ¨¡å‹ä¸ºä¾‹ï¼Œé¦–å…ˆä¸‹è½½[é¢„è®­ç»ƒLLaMAæƒé‡](https://huggingface.co/decapoda-research/llama-7b-hf)ï¼Œè½¬æ¢åˆ°TencentPretrainæ ¼å¼ï¼š

```
python3 scripts/convert_llama_from_huggingface_to_tencentpretrain.py --input_model_path $LLaMA_HF_PATH \
                       --output_model_path  models/llama-7b.bin --type 7B
```

ä¸‹è½½[ä¸­æ–‡é¢„è®­ç»ƒè¯­æ–™](corpus/README.md)ï¼Œ
é¢„å¤„ç†ï¼š

```
python3 preprocess.py --corpus_path $CORPUS_PATH --spm_model_path $LLaMA_PATH/tokenizer.model \
                      --dataset_path $OUTPUT_DATASET_PATH --data_processor lm --seq_length 512
```

é¢„è®­ç»ƒï¼š

```
deepspeed pretrain.py --deepspeed --deepspeed_config models/deepspeed_zero3_config.json \
                      --pretrained_model_path models/llama-7b.bin \
                      --dataset_path $OUTPUT_DATASET_PATH --spm_model_path $LLaMA_PATH/tokenizer.model \
                      --config_path models/llama/7b_config.json \
                      --output_model_path models/llama_zh_7b \
                      --world_size 8 --data_processor lm \
                      --total_steps 300000 --save_checkpoint_steps 5000 --batch_size 24
```



### ä¸­æ–‡æŒ‡ä»¤å­¦ä¹ 

æ„å»º[æŒ‡ä»¤æ•°æ®é›†](instructions/README.md)å¹¶é¢„å¤„ç†ï¼š

```
python3 preprocess.py --corpus_path $INSTRUCTION_PATH --spm_model_path $LLaMA_PATH/tokenizer.model \
                      --dataset_path $OUTPUT_DATASET_PATH --data_processor alpaca --seq_length 512
```

æŒ‡ä»¤å¾®è°ƒï¼š

```
deepspeed pretrain.py --deepspeed --deepspeed_config models/deepspeed_zero3_config.json \
                      --pretrained_model_path models/llama_zh_7b.bin \
                      --dataset_path $OUTPUT_DATASET_PATH --spm_model_path $LLaMA_PATH/tokenizer.model \
                      --config_path models/llama/7b_config.json \
                      --output_model_path models/chatllama_7b \
                      --world_size 8 --data_processor alpaca \
                      --total_steps 20000 --save_checkpoint_steps 2000 --batch_size 24
```



## ç”Ÿæˆç¤ºä¾‹


<details>
<summary><b>æ¨è/é—®ç­”</b></summary>

> **ç”¨æˆ·ï¼š** ä¸Šæµ·æœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹ï¼Ÿ
> 
> **ChatLLaMAï¼š** ä¸Šæµ·æœ‰è®¸å¤šå¥½ç©çš„åœ°æ–¹ï¼Œæ‚¨å¯ä»¥å»ä¸Šæµ·è¿ªå£«å°¼åº¦å‡åŒºã€åŸéšåº™ã€å¤–æ»©å’Œå—äº¬è·¯æ­¥è¡Œè¡—ç­‰æ™¯ç‚¹ã€‚æ­¤å¤–ï¼Œæ‚¨è¿˜å¯ä»¥å°è¯•å“å°å½“åœ°çš„ç¾é£Ÿï¼ŒåŒ…æ‹¬å°ç¬¼åŒ…ã€ç”Ÿç…åŒ…å’Œæµ·é²œç­‰ã€‚

>**ç”¨æˆ·ï¼š** ç»™æˆ‘ä»‹ç»ä¸€æœ¬å…³äºå“²å­¦çš„å¥½ä¹¦ã€‚
>
> **ChatLLaMAï¼š** æˆ‘å¯¹å“²å­¦æ„Ÿå…´è¶£ï¼Œä½†æ˜¯æˆ‘æ²¡æœ‰ç»éªŒæˆ–è€…çŸ¥è¯†æ¥ç†è§£è¿™ä¸ªé¢†åŸŸã€‚

>**ç”¨æˆ·ï¼š** ç»™æˆ‘æ¨èä¸€æ¬¾é€‚åˆåœ¨å®¶é”»ç‚¼çš„è¿åŠ¨å™¨æã€‚
>
> **ChatLLaMAï¼š** ä½ å¯ä»¥è€ƒè™‘è´­ä¹°ä¸€å°å“‘é“ƒå’Œä¸€ä¸ªç‘œä¼½å«æ¥åˆ¶ä½œä½ è‡ªå·±çš„è®­ç»ƒè®¡åˆ’ã€‚å“‘é“ƒå¯ä»¥å¸®åŠ©ä½ è¿›è¡Œè®¸å¤šä¸åŒçš„é”»ç‚¼ï¼Œä¾‹å¦‚è‚©éƒ¨ã€èƒŒéƒ¨ã€æ‰‹è‡‚ã€è…¹éƒ¨å’Œè…¿éƒ¨ã€‚ç‘œä¼½å«å¯ä»¥å¸®åŠ©ä½ è·µè¡Œç‘œä¼½ï¼Œæé«˜æŸ”éŸ§æ€§å’Œå¹³è¡¡æ€§ã€‚ä½ å¯ä»¥é€šè¿‡ç½‘ä¸Šè´­ä¹°è¿™äº›å™¨æï¼Œæˆ–è€…å‰å¾€å½“åœ°çš„å¥èº«æˆ¿æˆ–è¿åŠ¨å™¨æåº—é€‰æ‹©é€‚åˆä½ çš„å™¨æã€‚

>**ç”¨æˆ·ï¼š** ç»™æˆ‘æ¨èä¸€å®¶é™„è¿‘çš„è¥¿é¤å…ã€‚æˆ‘çš„ä½ç½®æ˜¯æ·±åœ³å¸‚å—å±±åŒºç²¤æµ·è¡—é“ã€‚
>
> **ChatLLaMAï¼š** æ¨èè¾°æ˜Ÿé¤å…ï¼Œåœ°å€ï¼šæ·±åœ³å¸‚å—å±±åŒºé¦™æ´²æµ·æ¹¾è¾°æ™¯èŠ±å›­BåŒº2Fï¼Œé è¿‘ç²¤æµ·è¡—é“ï¼Œä¸ºæ‚¨æä¾›ç¾å‘³çš„è¥¿é¤å’Œç²¾è‡´çš„æœåŠ¡ã€‚æ­¤é¤å…çš„ç‰¹è‰²æ˜¯æä¾›åˆ›æ„ç¾é£Ÿå’Œæ—¶å°šçš„é¤é¥®ç¯å¢ƒã€‚ç‰¹è‰²èœåŒ…æ‹¬é¦™ç…ç‰›æ’ã€æµ·é²œæŠ«è¨å’ŒèŠå£«é¾™è™¾æ„é¢ç­‰ã€‚æœåŠ¡å‘˜çƒ­æƒ…å‘¨åˆ°ï¼Œé¤å…ç»è¥æ—¶é—´ä¸ºæ™šé¤è‡³å‡Œæ™¨2ç‚¹ã€‚ä»¥ä¸‹æ˜¯æ›´å¤šé™„è¿‘è¥¿é¤å…çš„æ¨èï¼š1. Fleming's Steakhouse & Wine Barï¼šåœ°å€ï¼šæ·±åœ³å¸‚ç¦ç”°åŒºåå¼ºå—è·¯8å·

</details>


<details>
<summary><b>æœºå™¨ç¿»è¯‘</b></summary>

> **ç”¨æˆ·ï¼š** ç¿»è¯‘æˆè‹±æ–‡ï¼šå¤§é‡ç‹¬ç”Ÿå­å¥³çš„å‡ºç°ï¼Œä¸ä»…ä½¿ä¸­å›½çš„å®¶åº­ç»“æ„å‘ç”Ÿäº†å˜åŒ–ï¼Œè€Œä¸”ä¹Ÿå¸¦æ¥äº†ä¸€äº›ä»¤äººæ‹…å¿§çš„ç¤¾ä¼šé—®é¢˜ã€‚
>
> **ChatLLaMAï¼š** It led to the increase of the number of single-child families and also to some social problems in China.

> **ç”¨æˆ·ï¼š** ç¿»è¯‘æˆæ—¥è¯­ï¼šæ¯ä¸€ä¸ªæˆå°±éƒ½å§‹äºå‹‡äºå°è¯•çš„å†³å¿ƒã€‚
>
> **ChatLLaMAï¼š** å¤§åˆ‡ãªã“ã¨ã‚’ã™ã‚‹ã“ã¨ã§ã€ã©ã‚“ãªæˆå°±ãŒã‚‚ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

</details>


<details>
<summary><b>æ•°å­¦/ä»£ç </b></summary>

> **ç”¨æˆ·ï¼š** åˆ›å»ºä¸€ä¸ªåŒ…å«åä¸ªå…ƒç´ çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯æ–æ³¢é‚£å¥‘æ•°åˆ—ä¸­å‰åä¸ªæ•°å­—ä¹‹ä¸€ã€‚
>
> **ChatLLaMAï¼š** ä»¥ä¸‹æ˜¯æ–æ³¢é‚£å¥‘æ•°åˆ—ä¸­å‰åä¸ªæ•°å­—ï¼š 0ï¼Œ1, 1, 2, 3, 5, 8, 13, 21, 34

> **ç”¨æˆ·ï¼š** è¾“å…¥ä¸¤ä¸ªæ•°å­—ï¼Œè¿”å›å®ƒä»¬ä¹‹é—´çš„æ‰€æœ‰è´¨æ•°ã€‚ 3, 31 ã€‚
>
> **ChatLLaMAï¼š** 3, 3, 5, 7, 11, 13, 17

> **ç”¨æˆ·ï¼š** å†™ä¸€ä¸ªpythonå‡½æ•°åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­æ‰¾å‡ºåªå‡ºç°ä¸€æ¬¡çš„æ•°å­—ã€‚
>
> **ChatLLaMAï¼š** å¯ä»¥ä½¿ç”¨é›†åˆæ¥å®ç°ã€‚å°†å‡½æ•°ä¼ é€’éœ€è¦æœç´¢çš„åˆ—è¡¨å¹¶å®ç°â€œåªå‡ºç°ä¸€æ¬¡â€çš„æ¡ä»¶ã€‚ç„¶åä½¿ç”¨â€œé›†åˆâ€æ¸è¿›å¼è¾¨è¯†æœç´¢çš„æ•°å­—ï¼Œå¹¶è¿”å›ç»“æœã€‚ä¼ªä»£ç å¦‚ä¸‹ï¼š
```python
python def single_digit(lst):
     seen = set()
     for num in lst:
         if seen.add(num) and num != seen:
             return list(seen)
     return [] 
```
> è°ƒç”¨ç¤ºä¾‹ï¼š
> lst = [4, 2, 5, 4, 1, 4, 6, 5, 4, 8, 4]  
> res = single_digit(lst)  
> print(res)  
> è¾“å‡º [4, 1]

</details>

<details>
<summary><b>æœºå™¨å†™ä½œ</b></summary>

> **ç”¨æˆ·ï¼š**  ç¼–å†™ä¸€åˆ™å…³äºç§‘æŠ€æ–°é—»çš„æ–°é—»ç¨¿ã€‚ä»Šå¤©å°ç±³å…¬å¸æ¨å‡ºäº†ä¸€æ¬¾å…¨æ–°çš„æ™ºèƒ½æ‰‹è¡¨ï¼Œé…å¤‡äº†å„ç§å¥åº·å’Œå¨±ä¹åŠŸèƒ½ã€‚è¯·ç¼–å†™ä¸€åˆ™æ–°é—»ç¨¿ï¼Œä»‹ç»è¿™æ¬¾æ™ºèƒ½æ‰‹è¡¨çš„ç‰¹ç‚¹ï¼ŒåŠŸèƒ½å’Œä»·æ ¼ã€‚
>
> **ChatLLaMAï¼š** æ®å°ç±³å…¬å¸é€éœ²ï¼Œè¿™æ¬¾æ™ºèƒ½æ‰‹è¡¨åä¸ºâ€œå°ç±³è¶…çº§æ‰‹è¡¨â€ï¼Œå°†äºä»Šå¤©æ™šé—´åœ¨åŒ—äº¬äº®ç›¸ã€‚è¯¥æ‰‹è¡¨é‡‡ç”¨äº†æœ€æ–°çš„èŠ¯ç‰‡å’Œäººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œæ”¯æŒå¤šé¡¹å¥åº·å’Œå¨±ä¹åŠŸèƒ½ã€‚å…¶ä¸­ï¼Œå°ç±³Supermicroé€šè¿‡äººå·¥æ™ºèƒ½ç›‘æµ‹ç”¨æˆ·çš„å¥åº·çŠ¶å†µï¼ŒåŒ…æ‹¬å¿ƒç‡ã€è¡€å‹ã€ç¡çœ è´¨é‡ç­‰æŒ‡æ ‡ï¼Œå¹¶ç»™å‡ºç›¸åº”çš„å»ºè®®ã€‚åŒæ—¶ï¼Œè¯¥æ‰‹è¡¨è¿˜æ”¯æŒè¯­éŸ³åŠ©æ‰‹ã€æ™ºèƒ½æé†’ã€åœ°å›¾å¯¼èˆªç­‰åŠŸèƒ½ã€‚å¦å¤–ï¼Œå°ç±³Supermicroçš„ä»·æ ¼ä¸º2999å…ƒäººæ°‘å¸ï¼Œå°†äº11æœˆ11æ—¥åœ¨å°ç±³å®˜ç½‘å’Œçº¿ä¸‹å•†åŸæ­£å¼ä¸Šå¸‚ã€‚

> **ç”¨æˆ·ï¼š** æ ¹æ®æ ‡é¢˜å†™è®ºæ–‡æ‘˜è¦ï¼šåœ°ç¼˜æ”¿æ²»è§†é‡ä¸­çš„ä¸­äºšåŠå…¶å¯¹ä¸­å›½çš„å½±å“ã€‚
>
> **ChatLLaMAï¼š** ä¸­äºšçš„åœ°ç¼˜æ”¿æ²»ç‰¹æ®Šæ€§ä½¿å…¶æˆä¸ºä¿ƒè¿›ä¸­å›½ä¸å‘¨è¾¹å›½å®¶å…³ç³»å‘å±•çš„é‡è¦å¹³å°.ä¸­äºšä¸åŒå›½å®¶ä¹‹é—´çš„åœ°ç¼˜å…³ç³»å’Œå†å²æ–‡åŒ–ä¼ ç»Ÿä½¿å…¶åœ¨å›½é™…æ”¿æ²»èˆå°ä¸Šå…·æœ‰ä¸åŒçš„è§’è‰²å’Œå­˜åœ¨çš„é—®é¢˜.ä»¥ä¿„ç½—æ–¯ä¸ºä¾‹,å®ƒåœ¨å³ä½çš„åœ°ç¼˜ä½ç½®ä½¿å…¶æˆä¸ºä¸œæ¬§å’Œä¸­äºšçš„åœ°ç¼˜å¤§å›½,ä½†ç”±äºå…¶å†å²æ–‡åŒ–å’Œæ°‘æ—ç»“æ„çš„ç‰¹æ®Šæ€§,å…¶åœ¨å›½é™…ä¸Šçš„åœ°ä½å’Œå½±å“åŠ›å—åˆ°é™åˆ¶,å› è€Œå…¶å¯¹ä¸­å›½çš„å½±å“ä¸»è¦ä½“ç°åœ¨ä¸œæ¬§åœ°åŒº.ç¾å›½ç”±äºå…¶åœ°ç¼˜ä½ç½®å’Œèƒ½åŠ›çš„ä¼˜åŠ¿,å¯¹ä¸­äºšåœ°åŒºçš„å½±å“ä¹Ÿæ¯”è¾ƒæ˜¾è‘—,å®ƒçš„å›½å®¶åˆ©ç›Šå’Œæ”¿æ²»è§’è‰²ä½¿å…¶åœ¨ä¸­äºšåœ°åŒºçš„å½±å“åŠ›ä¸æ–­æ‰©å¼ ,ç‰¹åˆ«æ˜¯åœ¨ Turkmenistanã€Kazakhstanã€Uzbekistan ä¸‰ä¸ªâ€œæ–°å…´å¸‚åœºâ€çš„åœ°åŒº,å…¶é€šè¿‡æä¾›ç»æµã€æ”¿æ²»å’Œå†›äº‹æ´åŠ©æ¥å¡‘é€ å½“åœ°çš„...

> **ç”¨æˆ·ï¼š** å†™ä¸€é¦–å…³äºåŒ—äº¬çš„å†¬å¤©çš„è¯—ã€‚
>
> **ChatLLaMAï¼š** åŸå¸‚ä¸­å°½æ˜¯çœ¼è§’çš„ç§¯é›ªã€‚   
éœœå‡Œå¤©ç©ºä¸­ä»¿ä½›è¢«ä¸€å±‚å±‚çƒ™å°ï¼Œ  
å†ç»å¾ˆå¤šå¹´çš„å†å²ä¸æ–‡åŒ–ï¼Œ  
è¿™ä¸ªåŸå¸‚æ…¢æ…¢åœ°èåŒ–åœ¨å¤§åœ°é‡Œã€  
æˆä¸ºäº†è¿™ä¸ªå¤§éƒ½å¸‚çš„ä¸€éƒ¨åˆ†ã€‚  
ç–²æƒ«çš„äººä»¬åœ¨å†¬å¤©è£¹ä¸Šæ¯›è¡£ï¼Œ  
æ‰“è½¦ç©¿æ¢­ç€ï¼Œ  
å¯»æ‰¾é¿é£å±…æ‰€ï¼Œ  
æ¯ä¸€ä¸ªè§’è½éƒ½è•´å«ç€å¿ƒä¸­çš„å¯‚é™ï¼Œ  
æˆä¸ºäº†è¿™ä¸ªåŸå¸‚çš„èˆ’é€‚æ„Ÿï¼Œ  
æ„Ÿå—ç€å†å²æ²‰æ·€çš„åšé‡ã€‚  
å½“å¤ªé˜³å‡èµ·æ—¶ï¼Œ  
åŸå¸‚é‡æ–°æ”¶ç¼©å›æ¸”èˆ¹ï¼Œ  
æ¯ä¸€ä¸ªè§’åº¦éƒ½è¢«æ²‰é»˜ï¼Œ  

</details>

<details>
<summary><b>ä¸ ChatGPT å¯¹æ¯”</b></summary>

æˆ‘ä»¬é€‰æ‹©äº†å‡ ä¸ªè§’åº¦åˆé’»çš„ä¸­æ–‡æ ·æœ¬ï¼Œå¯¹æ¯” ChatLLaMA å’Œ ChatGPT çš„å›ç­”ã€‚

**ChatLLaMA:**

> ä¸­æ–‡åˆ†è¯ï¼šæ— çº¿ç”µæ³•å›½åˆ«ç ”ç©¶
>
> åˆ†è¯ç»“æœåˆ†åˆ«ä¸ºï¼šæ— çº¿ç”µï¼Œæ³•å›½ï¼Œåˆ«ç ”ç©¶ã€‚


> æ¥åˆ°æ¨è¿‡æ›¾ç»ç”Ÿæ´»è¿‡çš„åœ°æ–¹ï¼Œå°é¾™å¥³åŠ¨æƒ…åœ°è¯´ï¼šâ€œæˆ‘ä¹Ÿæƒ³è¿‡è¿‡è¿‡å„¿è¿‡è¿‡çš„ç”Ÿæ´»ã€‚â€
> 
> æ¨è¿‡çš„å±…æ‰€åœ¨åƒå¹´å¤é•‡èµµå·æ¡¥çš„è¥¿ç«¯ï¼Œå°é¾™å¥³å¬å®‹è¯â€œèµµå·æ¡¥æ—¥ç…§é»„é—¨åŸâ€ï¼Œé©¬ä¸Šæƒ³åˆ°äº†è‡ªå·±çš„æ¢¦æƒ³ã€‚åœ¨åƒå¹´å¤é•‡çœ‹ç€å¤è‰²å¤é¦™çš„å®«æ®¿ï¼Œå°é¾™å¥³ä¹Ÿæƒ³åˆ°æ›¾ç»ç”Ÿæ´»çš„æ¨è¿‡ã€‚

>  å¯¹å™æ‰“å‡»æ˜¯ä¸€æ¬¡æ€§è¡Œä¸ºï¼Ÿ
> 
> å¯¹å™æ‰“å‡»æ˜¯æŒ‡ä¸€æ¬¡æ€§çš„æŸç§å†›äº‹è¡ŒåŠ¨ï¼Œå®ƒçš„ä¸»è¦ç›®çš„æ˜¯æ‰“å‡»å—åˆ°å¨èƒçš„å›½å®¶æˆ–åœ°åŒºã€‚è¿™æ„å‘³ç€è¿™äº›è¡ŒåŠ¨åœ¨ä¸€ä¸ªç‰¹å®šæ—¶é—´å’Œåœ°ç‚¹å†…é‡‡å–ï¼Œå¹¶ä¸”æœ‰æ˜ç¡®çš„ç›®çš„å’Œç»“æŸæ—¶é—´ã€‚

>  ã€Šç»¿æ—ä¿Šæ°ã€‹--æ—ä¿Šæ°åšé”™äº†ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆè¦ç»¿ä»–
> 
> æ—ä¿Šæ°åšé”™äº†ä¸»æŒèŠ‚ç›®ã€Šç»¿æ—ä¿Šæ°ã€‹çš„äº‹æƒ…è™½ç„¶è¢«å¾ˆå¤šäººæåŠå’Œä¸æ–­è¢«æ¶‰åŠï¼Œä½†ç›®å‰ä»æ²¡æœ‰å®˜æ–¹ç»“è®ºå’Œè§£ã€‚

> å¦‚ä½•è§£é‡Šè¿™å¥è¯ï¼šè¿‡å‡ å¤©å¤©å¤©å¤©æ°”ä¸å¥½
> 
> è¿™å¥è¯ä¼ è¾¾äº†å¤©æ°”æƒ…å†µæ€»æ˜¯ä¸å¥½çš„æ„Ÿè§‰ã€‚

**ChatGPT:**

![](assets/chatgpt_1.jpg)

![](assets/chatgpt_2.jpg)

![](assets/chatgpt_3.jpg)

![](assets/chatgpt_4.jpg)

![](assets/chatgpt_5.jpg)

</details>

## å±€é™æ€§

ChatLLaMA å®Œå…¨åŸºäºç¤¾åŒºå¼€æ”¾è¯­æ–™è®­ç»ƒï¼Œå†…å®¹æœªç»äººå·¥ä¿®æ­£ã€‚å—é™äºæ¨¡å‹å’Œè®­ç»ƒæ•°æ®è§„æ¨¡ï¼ŒChatLLaMA çš„è¯­è¨€èƒ½åŠ›è¾ƒå¼±ï¼Œ
åœ¨å¤šè½®å¯¹è¯ã€é€»è¾‘æ¨ç†ã€çŸ¥è¯†é—®ç­”ç­‰åœºæ™¯å…·æœ‰æ˜æ˜¾ç¼ºé™·ï¼Œä¹Ÿå¯èƒ½äº§ç”Ÿå¸¦æœ‰åè§æˆ–æœ‰å®³å†…å®¹ã€‚

## ä¸­æ–‡é¢„è®­ç»ƒ/æŒ‡ä»¤æ•°æ®é›†

æ±‡æ€»å¼€æºç¤¾åŒºæ„å»ºçš„ä¸­æ–‡æŒ‡ä»¤å­¦ä¹ æ•°æ®ï¼Œå¹¶è½¬æ¢æˆç»Ÿä¸€æ ¼å¼ç”¨äºæŒ‡ä»¤å¾®è°ƒã€‚ 

[æ„å»ºä¸­](instructions/README.md)


## äº¤æµå’Œé—®é¢˜åé¦ˆ

ç”±äºå¾®ä¿¡ç¾¤è¾¾åˆ°äººæ•°ä¸Šé™ï¼Œæœç´¢å¾®ä¿¡å· chatllamaï¼Œæ·»åŠ ä¸ºå¥½å‹åæ‹‰å…¥ç¾¤èŠã€‚

## TODO List

- [ ] HuggingFace è½¬æ¢è„šæœ¬å’Œæƒé‡ä¸Šä¼ 
- [ ] æ”¯æŒé‡åŒ–æ¨¡å‹ CUDA éƒ¨ç½²
- [ ] ChatLLaMA é¢†åŸŸé€‚åº”æ¡ˆä¾‹
- [ ] å¼ºåŒ–å­¦ä¹ 

## License

Our code and documents are released under Apache Licence 2.0

Following LLaMA, our pre-trained weights are released under GNU General Public License v3.0

## Contributors

We thank contributors for both [TencentPretrain](https://github.com/Tencent/TencentPretrain) and Chinese-ChatLLaMA projects.

Authors: [Yudong Li](https://github.com/ydli-ai), [Zhe Zhao](https://github.com/zhezhaoa), [Yuhao Feng](https://github.com/fengyh3), [Cheng Hou](https://github.com/hhou435), [Shuang Li](https://github.com/thulishuang), [Hao Li](https://github.com/wmpscc), [Xianxu Hou](https://houxianxu.github.io/)

Corresponding Authors: [Linlin Shen](https://scholar.google.com/citations?user=AZ_y9HgAAAAJ&hl=zh-CN), Kimmo Yan

